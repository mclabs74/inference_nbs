{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pretrained_pil_inference",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5auCaWFcSnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fba2fa4-800d-4bdc-8c10-62c4f6a9314f"
      },
      "source": [
        "!pip install -q git+https://github.com/matthewchung74/inference_params.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 62 kB 788 kB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 14.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 46.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 60.8 MB/s \n",
            "\u001b[?25h  Building wheel for inference-params (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAk2UoWhnFsw"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import io\n",
        "import base64\n",
        "import json\n",
        "\n",
        "from inference_params.inference_params import inference_test, FieldType, inference_predict\n",
        "\n",
        "# text input will be the label for the app input\n",
        "input = {\"input\": FieldType.Text}\n",
        "# result will be the label for the app output\n",
        "output = {\"result\": FieldType.Text}\n",
        "\n",
        "@inference_predict(input=input, output=output)\n",
        "def predict(params):\n",
        "    input = params['input']\n",
        "    return {\"result\": input}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2BtcXbdcZX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176cca0a-949e-4d62-9d9f-6f721d5e861e"
      },
      "source": [
        "from inference_params.inference_params import in_colab\n",
        "\n",
        "if in_colab():\n",
        "    inference_test(predict_func=predict, params={'input': 'my input'})"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote results to result.json duration: 1e-06 seconds\n",
            "Please take a look and verify the results\n",
            "{\n",
            "    \"result\": \"my input\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIZh4SfkoZXv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}