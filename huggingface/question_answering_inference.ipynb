{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "question_answering_inference",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mclabs74/inference_nbs/blob/dev/huggingface/question_answering_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def create_requirements_file():\n",
        "    if not Path(\"requirements.txt\").exists():\n",
        "        requirements = [\"transformers\"]\n",
        "        with open('requirements.txt', 'w') as filehandle:\n",
        "            for listitem in requirements:\n",
        "                filehandle.write('%s\\n' % listitem)\n",
        "    \n",
        "create_requirements_file()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3mH8SFV9_0d"
      },
      "source": [
        "!wget -q https://ml-inference.s3-us-west-2.amazonaws.com/hugging_face_question_answering_training.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RMA1Zgi9_3O"
      },
      "source": [
        "!unzip hugging_face_question_answering_training.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7gb4NeD-2jm"
      },
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "model_path = Path(\"model\")\n",
        "\n",
        "tokenizer_new = AutoTokenizer.from_pretrained(str(model_path/\"tokenizer\"), config=AutoConfig.from_pretrained(model_path))\n",
        "model_new = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "model_new.cpu();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnrVk99Cfhux"
      },
      "source": [
        "!pip install -q git+https://github.com/matthewchung74/inference_params.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWEZVpuW9_8o"
      },
      "source": [
        "from inference_params.inference_params import inference_test, FieldType, inference_predict\n",
        "\n",
        "# text input will be the label for the app input\n",
        "input = {\"question\": FieldType.Text, \"context\": FieldType.Text}\n",
        "# result will be the label for the app output\n",
        "output = {\"result\": FieldType.Text}\n",
        "\n",
        "@inference_predict(input=input, output=output)\n",
        "def predict(inputs):\n",
        "    question = inputs[\"question\"]\n",
        "    context = inputs[\"context\"]\n",
        "    inputs = tokenizer_new.encode_plus(question, context,add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    output = model_new(**inputs)\n",
        "    answer_start_scores = output.start_logits\n",
        "    answer_end_scores = output.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    answer = tokenizer_new.convert_tokens_to_string(tokenizer_new.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return {\"result\": answer}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "source": [
        "from inference_params.inference_params import in_colab\n",
        "\n",
        "if in_colab():\n",
        "    context = r\"\"\"\n",
        "    ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "    architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
        "    Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "    TensorFlow 2.0 and PyTorch.\n",
        "    \"\"\"\n",
        "\n",
        "    params = {\"question\":\"How many pretrained models are available in Transformers?\",\n",
        "            \"context\":context}\n",
        "\n",
        "    inference_test(predict_func=predict, params=params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1cRH09NgGFb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}