{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "question_answering_inference",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewchung74/inference_nbs/blob/main/huggingface/question_answering_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZpLpwqkif0I"
      },
      "source": [
        "this_is_main_branch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def create_requirements_file():\n",
        "    if not Path(\"requirements.txt\").exists():\n",
        "        requirements = [\"transformers\"]\n",
        "        with open('requirements.txt', 'w') as filehandle:\n",
        "            for listitem in requirements:\n",
        "                filehandle.write('%s\\n' % listitem)\n",
        "    \n",
        "create_requirements_file()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvslsfMIrIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64692079-e9a8-43bd-9cca-c38b61597e37"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->-r requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=3cbd6992f29bb2e93e2bbfb23dfdb135de50019a078e3e56c30c85e6c2e8fab2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3mH8SFV9_0d",
        "outputId": "e5e64386-7882-41ec-c08e-242ed73bf4fe"
      },
      "source": [
        "!wget https://ml-inference.s3-us-west-2.amazonaws.com/hugging_face_question_answering_training.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-14 17:46:39--  https://ml-inference.s3-us-west-2.amazonaws.com/hugging_face_question_answering_training.zip\n",
            "Resolving ml-inference.s3-us-west-2.amazonaws.com (ml-inference.s3-us-west-2.amazonaws.com)... 52.218.242.249\n",
            "Connecting to ml-inference.s3-us-west-2.amazonaws.com (ml-inference.s3-us-west-2.amazonaws.com)|52.218.242.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 244710271 (233M) [application/zip]\n",
            "Saving to: ‘hugging_face_question_answering_training.zip’\n",
            "\n",
            "hugging_face_questi 100%[===================>] 233.37M  66.0MB/s    in 3.5s    \n",
            "\n",
            "2021-04-14 17:46:42 (66.0 MB/s) - ‘hugging_face_question_answering_training.zip’ saved [244710271/244710271]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RMA1Zgi9_3O",
        "outputId": "77eafa34-8a67-440a-8f15-b5ba9297d19a"
      },
      "source": [
        "!unzip hugging_face_question_answering_training.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  hugging_face_question_answering_training.zip\n",
            "   creating: model/\n",
            "  inflating: model/config.json       \n",
            "   creating: model/tokenizer/\n",
            "  inflating: model/tokenizer/vocab.txt  \n",
            "  inflating: model/pytorch_model.bin  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7gb4NeD-2jm"
      },
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "model_path = Path(\"model\")\n",
        "\n",
        "tokenizer_new = AutoTokenizer.from_pretrained(str(model_path/\"tokenizer\"), config=AutoConfig.from_pretrained(model_path))\n",
        "model_new = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "model_new.cpu();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnrVk99Cfhux",
        "outputId": "5eab7da5-9290-40b8-9e4a-297508dd3264"
      },
      "source": [
        "!pip install -q git+https://github.com/matthewchung74/inference_params.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for inference-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWEZVpuW9_8o"
      },
      "source": [
        "from inference_params.inference_params import inference_test, FieldType, inference_predict\n",
        "\n",
        "# text input will be the label for the app input\n",
        "input = {\"question\": FieldType.Text, \"context\": FieldType.Text}\n",
        "# result will be the label for the app output\n",
        "output = {\"result\": FieldType.Text}\n",
        "\n",
        "@inference_predict(input=input, output=output)\n",
        "def predict(inputs):\n",
        "    question = inputs[\"question\"]\n",
        "    context = inputs[\"context\"]\n",
        "    inputs = tokenizer_new.encode_plus(question, context,add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    output = model_new(**inputs)\n",
        "    answer_start_scores = output.start_logits\n",
        "    answer_end_scores = output.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    answer = tokenizer_new.convert_tokens_to_string(tokenizer_new.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return {\"result\": answer}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreSlFmlIrIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e1cc84-f1b1-49d5-fc22-ab3f9c70096f"
      },
      "source": [
        "context = r\"\"\"\n",
        "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "\n",
        "inputs = {\"question\":\"How many pretrained models are available in Transformers?\",\n",
        "          \"context\":context}\n",
        "\n",
        "print(inputs[\"question\"])\n",
        "print(predict(inputs))\n",
        "\n",
        "inputs = {\"question\":\"What does Transformers provide?\",\n",
        "          \"context\":context}\n",
        "\n",
        "print(inputs[\"question\"])\n",
        "print(predict(inputs))\n",
        "\n",
        "inputs = {\"question\":\"Transformers provides interoperability between which frameworks?\",\n",
        "          \"context\":context}\n",
        "\n",
        "print(inputs[\"question\"])\n",
        "print(predict(inputs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How many pretrained models are available in Transformers?\n",
            "({'result': 'over 32 +'}, 0.23412585258483887)\n",
            "wrote results to result.json duration: 0.239064 seconds\n",
            "What does Transformers provide?\n",
            "({'result': 'general - purpose architectures'}, 0.22880816459655762)\n",
            "Transformers provides interoperability between which frameworks?\n",
            "({'result': 'tensorflow 2. 0 and pytorch'}, 0.22966623306274414)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvp1yQWKALBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b924858d-fea0-4d27-9550-93bd46fc5154"
      },
      "source": [
        "inference_test(predict_func=predict, params=inputs)\n",
        "!cat result.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wrote results to result.json duration: 0.234966 seconds\n",
            "{\"result\": \"tensorflow 2. 0 and pytorch\"}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1cRH09NgGFb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}